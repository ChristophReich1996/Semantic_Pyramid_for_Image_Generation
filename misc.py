from typing import List, Tuple

import torch
import torch.nn.functional as F
import numpy as np
from skimage.draw import random_shapes


def get_masks_for_training(
        mask_shapes: List[Tuple] =
        [(64, 128, 128), (128, 64, 64), (256, 32, 32), (512, 16, 16), (512, 8, 8), (4096,), (1000,)],
        device: str = 'cpu', add_batch_size: bool = False,
        p_layers: List[float] = [0.25, 0.2, 0.15, 0.1, 0.1, 0.1, 0.1],
        p_random_mask: float = 0.3) -> List[torch.Tensor]:
    '''
    Method returns random masks similar to 3.2. of the paper
    :param mask_shapes: (List[Tuple]) Shapes of the features generated by the vgg16 model
    :param device: (str) Device to store tensor masks
    :param add_batch_size: (bool) If true a batch size is added to each mask
    :param p_layers: (List[float]) Probability distribution of which layer to chose to start random masking
    :param p_random_mask: (float) Probability that a random mask is generated else no mask is utilized
    :return: (List[torch.Tensor]) Generated masks for each feature tensor
    '''
    # Select layer where no masking is used. Every output from the deeper layers get mapped out. Every higher layer gets
    # masked by a random shape
    selected_layer = np.random.choice(range(7), p=p_layers)
    # Make masks
    masks = []
    random_mask = None
    random_mask_used = False
    for index, mask_shape in enumerate(reversed(mask_shapes)):
        # Full mask on case
        if index < selected_layer:
            if len(mask_shape) > 1:
                # Save mask to list
                masks.append(torch.zeros((1, mask_shape[1], mask_shape[2]), dtype=torch.float32, device=device))
            else:
                # Save mask to list
                masks.append(torch.zeros(mask_shape, dtype=torch.float32, device=device))
        # No mask case
        elif index == selected_layer:
            if len(mask_shape) > 1:
                # Save mask to list
                masks.append(torch.ones((1, mask_shape[1], mask_shape[2]), dtype=torch.float32, device=device))
            else:
                # Save mask to list
                masks.append(torch.ones(mask_shape, dtype=torch.float32, device=device))
        # Random mask cases
        elif index > selected_layer and random_mask is None:
            if len(mask_shape) > 2:
                # Get random mask
                if np.random.rand() < p_random_mask:
                    random_mask_used = True
                    random_mask = random_shapes(mask_shape[1:],
                                                min_shapes=1,
                                                max_shapes=4,
                                                min_size=min(8, mask_shape[1] // 2),
                                                allow_overlap=True)[0][:, :, 0]
                    # Random mask to torch tensor
                    random_mask = torch.tensor(random_mask, dtype=torch.float32, device=device)[None, :, :]
                    # Change range of mask to [0, 1]
                    random_mask = (random_mask == 255.0).float()
                else:
                    # Make no mask
                    random_mask = torch.ones(mask_shape[1:], dtype=torch.float32, device=device)[None, :, :]
                # Save mask to list
                masks.append(random_mask)
            else:
                # Save mask to list
                masks.append(torch.randint(low=0, high=2, size=mask_shape, dtype=torch.float32, device=device))
        else:
            # Save mask to list
            if random_mask_used:
                masks.append(F.upsample_nearest(random_mask[None, :, :, :], size=mask_shape[1:]).float().to(device)[0])
            else:
                masks.append(torch.ones(mask_shape[1:], dtype=torch.float32, device=device)[None, :, :])
    # Add batch size dimension
    if add_batch_size:
        for index in range(len(masks)):
            masks[index] = masks[index].unsqueeze(dim=0)
    # Reverse order of masks to match the features of the vgg16 model
    masks.reverse()
    return masks
